{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tmv9bz5v4q.us-east-1.awsapprunner.com/v3/text'\n",
    "token = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJhZG1pbiIsImlzX2FkbWluIjp0cnVlLCJleHAiOjE3MTQxNzIzODB9.2qI3F4f9crLJ1vCVNjjlQcm_c3h6dpscv-CQ78H5sSo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_id = 106\n",
    "text_name = 'swh-Neno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, params={'revision_id': revision_id}, headers={'Authorization': f'Bearer {token}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [{'vref': item['verse_reference'], 'text': item['text']} for item in response.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.DataFrame(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.to_csv(f'{text_name}_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = modal.Function.lookup('get-labse-embeddings', 'assess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = f.remote(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_df = pd.DataFrame(embeddings['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(em_df, on='vref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc.to_csv('BSB_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('BSB_embeddings.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/nn_embeddings_dict_swh-Neno_texts.json', 'r') as f:\n",
    "    embeddings_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_closest_words(target_word, embeddings_dict, top_n=10, use_cosine=True):\n",
    "    target_embedding = np.array(embeddings_dict[target_word])\n",
    "    distances = []\n",
    "\n",
    "    for word, embedding in embeddings_dict.items():\n",
    "        if word == target_word:\n",
    "            continue  # Skip the comparison with itself\n",
    "        \n",
    "        current_embedding = np.array(embedding)\n",
    "        if use_cosine:\n",
    "            distance = cosine(target_embedding, current_embedding)\n",
    "        else:\n",
    "            distance = euclidean(target_embedding, current_embedding)\n",
    "        \n",
    "        distances.append((word, distance))\n",
    "\n",
    "    # Sort by distance, lowest first if Euclidean, highest cosine similarity first\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Return the top n closest words\n",
    "    return distances[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atakayeketi', 0.503867982697475),\n",
       " ('zilianguka', 0.5271962680282616),\n",
       " ('akajitokeza', 0.5357676589725295),\n",
       " ('aliingia', 0.5373377180596268),\n",
       " ('walipita', 0.5412079750038833),\n",
       " ('akaenenda', 0.5481961380152648),\n",
       " ('kuenenda', 0.5483543644846882),\n",
       " ('nikaanguka', 0.5662270111836403),\n",
       " ('papo', 0.5672959633036945),\n",
       " ('wameingia', 0.5677313913134929)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word = \"kuingia\"\n",
    "find_top_closest_words(target_word, embeddings_dict, use_cosine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input/swh-Neno_texts.csv')\n",
    "em_df = pd.read_parquet('input/BSB_embeddings.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(em_df, on='vref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://ljirdl8ahafuab34.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": \"Bearer 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\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "\t\"parameters\": {}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': '503 Service Unavailable'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/.cache/pypoetry/virtualenvs/learning-embeddings-zJm_07dr-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "        AutoModelForSeq2SeqLM,\n",
    "        AutoTokenizer,\n",
    "        DataCollatorForSeq2Seq,\n",
    "        Seq2SeqTrainer,\n",
    "        Seq2SeqTrainingArguments,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        T5ForConditionalGeneration, \n",
    "        T5Tokenizer,\n",
    "    )\n",
    "from datasets import (\n",
    "        load_dataset, \n",
    "        Dataset\n",
    "    )\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "BASE_MODEL = \"jbochi/madlad400-3b-mt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_file_path = 'fine-tuned_models/data/aligned_embeddings_dict_MADLAD-kss-kss.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26651"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(emb_file_path) as f:\n",
    "    new_emb = json.load(f)\n",
    "\n",
    "new_vocab = [embedding['word'] for embedding in new_emb]\n",
    "embeddings = np.array([item['embedding'] for item in new_emb])\n",
    "\n",
    "tokenizer.add_tokens(list(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"ho̍ō ndu̍ chɔ̍m a̍\"\n",
    "\n",
    "# Preprocess the sentence to add ▁ at the beginning of each word\n",
    "preprocessed_sentence = ''.join(['▁' + word for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁ho̍ō▁ndu̍▁chɔ̍m▁a̍'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = tokenizer(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ho̍ō', 'ndu̍', 'chɔ̍m', 'a̍', '</s>']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(token) for token in sentence_tokens['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'fine-tuned_models/data/en-web-kisi.csv'\n",
    "df = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [805, 114648, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('▁hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Kisi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the beginning, God created the heavens and ...</td>\n",
       "      <td>Yāu̍wo̍ ho̍ō ndu̍ chɔ̍m a̍ mɛ̄ɛ̍ Mɛ̄lɛ̄ka̍ t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The earth was formless and empty. Darkness was...</td>\n",
       "      <td>nyɛ̄ o̍ nyɛ̄ wa̍ mu̍lu̍kɛ̄lɛ̍ɛ̄, ndu̍yɛ̍ nyɛ̄ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>God said, “Let there be light,” and there was ...</td>\n",
       "      <td>Mī Mɛ̄lɛ̄ka̍ dīmī āā, “Nyɛ̄ wa̍wo̍ŋndo̍ o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>God saw the light, and saw that it was good. G...</td>\n",
       "      <td>Mi̍ Mɛ̄lɛ̄ka̍ chā māā nyɛ̄ wa̍wo̍ŋ o̍ to̍o̍...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>God called the light “day”, and the darkness h...</td>\n",
       "      <td>Mbo̍ kē nyɛ̄ wa̍wo̍ŋndo̍ dīōla̍ŋ āā, īpa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  In the beginning, God created the heavens and ...   \n",
       "1  The earth was formless and empty. Darkness was...   \n",
       "2  God said, “Let there be light,” and there was ...   \n",
       "3  God saw the light, and saw that it was good. G...   \n",
       "4  God called the light “day”, and the darkness h...   \n",
       "\n",
       "                                                Kisi  \n",
       "0  Yāu̍wo̍ ho̍ō ndu̍ chɔ̍m a̍ mɛ̄ɛ̍ Mɛ̄lɛ̄ka̍ t...  \n",
       "1  nyɛ̄ o̍ nyɛ̄ wa̍ mu̍lu̍kɛ̄lɛ̍ɛ̄, ndu̍yɛ̍ nyɛ̄ ...  \n",
       "2  Mī Mɛ̄lɛ̄ka̍ dīmī āā, “Nyɛ̄ wa̍wo̍ŋndo̍ o...  \n",
       "3  Mi̍ Mɛ̄lɛ̄ka̍ chā māā nyɛ̄ wa̍wo̍ŋ o̍ to̍o̍...  \n",
       "4  Mbo̍ kē nyɛ̄ wa̍wo̍ŋndo̍ dīōla̍ŋ āā, īpa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list = []\n",
    "source_language = 'English'\n",
    "source_iso_code = 'eng'\n",
    "source_lang_code = 'en'\n",
    "target_language = 'Kisi'\n",
    "target_iso_code = 'kss'\n",
    "target_lang_code = 'ks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    pair_list.append(\n",
    "        {'id': i, \n",
    "        'translation': {source_lang_code: df.loc[i, source_language],\n",
    "                        target_lang_code: df.loc[i, target_language]}})\n",
    "\n",
    "random.Random(7).shuffle(pair_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = ''\n",
    "target_prefix = ''\n",
    "MAX_LENGTH = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang_code] for example in examples[\"translation\"]]\n",
    "    targets = [target_prefix + ''.join(['▁' + word for word in example[target_lang_code].lower().split()]) for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=MAX_LENGTH, truncation=True)\n",
    "    # print(model_inputs)\n",
    "    return model_inputs\n",
    "def gen():\n",
    "    for i in range(len(pair_list)):\n",
    "        yield {\"id\": pair_list[i]['id'], \"translation\": pair_list[i]['translation']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  48%|████▊     | 12000/24872 [00:07<00:06, 1916.78 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 24872/24872 [00:15<00:00, 1559.63 examples/s]\n",
      "Map: 100%|██████████| 6218/6218 [00:03<00:00, 1924.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_generator(gen)\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "tokenized_verses = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_verses['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'contributing to the needs of the saints, and given to hospitality.',\n",
       " 'ks': 'Tē chāa̍ŋāā nyāā wa̍nāā lāāla᷄ŋnda̍ ā wa̍ a̍ bā hāvɛ̍i̍yo̍ pɛ̍, ndu̍yɛ̍ mī lā nɔ̄ nyɛ̄ nda̍ nɔ̄ ko̍ŋ ba̍ hāvɛ̍i̍ ndɔ̄ɔ̍ wo̍ pɛ̍, lā ke̍ nda̍ ō ti᷄ŋ. Ndu̍yɛ̍ lā wa̍ lɔ̍ɔ̍ lɔ̍ɔ̍ a̍ kɔ̄ltā chɛ̄lāā le̍ mīālla̍ yīyāa̍ o̍ lā nya̍ lo̍ chi̍ɛ̍ī ni̍ŋ.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kɛ̍, mi̍ ī yɔ̄nu᷄ŋ a̍ bōōndi̍i̍ pi᷅lɛ̍, mi̍ pūa̍a̍ ā pūm ā cha᷄ŋ o̍ chē mi̍si̍ lo̍ ma̍ hūŋ a̍ ŋti̍ɔ̍ɔ̄. mɛ̄ɛ̍ pūa̍a̍ ha̍ā fūūlu̍u̍ dɔ̍ɔ̍, mi̍ pītɛ̍ pɛ̍ŋgi̍ bɛ̍ɛ̍ nyɛ̍ dīa̍a̍ dīo̍o̍ lātūlu̍ nda̍ wānāā lāāla᷄ŋnda̍ ha̍ā a̍ cho̍ a̍ chu̍ū le̍ wa̍. ndu̍ o̍o̍ chi̍ɛ̍ɛ̄li̍āŋ ndɛ̍ bɛ̍ɛ̍ a̍ nda̍ māā pāāndū le̍. le̍ sa̍bū ndu̍ o̍o̍ to̍sa̍ le̍nde̍ wo̍ cho̍ ni̍ māā sīōōŋnde̍ wa̍</s>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=9\n",
    "print(len(tokenized_verses['train'][i]['labels']))\n",
    "tokenizer.decode(tokenized_verses['train'][i]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aiuta'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(134508)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-embeddings-zJm_07dr-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
